{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a699a9fe",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc58e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168636a",
   "metadata": {},
   "source": [
    "# Download NLTK Library (if error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8cbb44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2285d8a2",
   "metadata": {},
   "source": [
    "# Settings Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd415b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "eng_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d4b5f",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738c2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document):\n",
    "    words = word_tokenize(document.lower())\n",
    "    words = [wnl.lemmatize(word) for word in words]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return {word : True for word in words if word.isalpha() and word not in eng_stopwords}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6f55c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd82be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "\n",
    "    dataset = pd.read_csv(\"./Dataset/Tweets.csv\")\n",
    "\n",
    "    feature_sets = [(preprocessing(text), label) for text, label in zip (dataset[\"text\"], dataset[\"airline_sentiment\"])]\n",
    "\n",
    "    split_index = int (len(feature_sets) * 0.85)\n",
    "\n",
    "    train_set, test_set = feature_sets[:split_index], feature_sets[split_index:]\n",
    "\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "    print (f\"Accuracy : {accuracy}\")\n",
    "\n",
    "    classifier.show_most_informative_features(5)\n",
    "\n",
    "    file = open(\"./model.pickle\", \"wb\")\n",
    "    pickle.dump(classifier, file)\n",
    "    file.close()\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def read_model():\n",
    "\n",
    "    try:\n",
    "        file = open(\"model.pickle\", \"rb\")\n",
    "        print (\"Model is available\")\n",
    "        print (\"Loading the model...\")\n",
    "        classifier = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        print (\"Model load successfully\")\n",
    "        classifier.show_most_informative_features(5)\n",
    "\n",
    "    except:\n",
    "        print (\"Model not available\")\n",
    "        print (\"Prepare for training\")\n",
    "        classifier = train_model()\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b824dfc",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f148981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_review():\n",
    "    while True:\n",
    "        review = input(\"Input your review [>= 2 words]\")\n",
    "        words = review.split()\n",
    "\n",
    "        if len(words) >= 2:\n",
    "            print (\"Review Added\")\n",
    "            return review\n",
    "        else:\n",
    "            print (\"Input must be more than 1 word\")\n",
    "\n",
    "def analyze_review(review, classifier):\n",
    "    \n",
    "    if len(review) == 0:\n",
    "        print (\"Review is empty\")\n",
    "        return\n",
    "\n",
    "    # Tokenizing\n",
    "    words = word_tokenize(review.lower())\n",
    "\n",
    "    # Frequency Distribution\n",
    "    words = FreqDist([word for word in words if word.isalpha() and word not in string.punctuation])\n",
    "\n",
    "    tagged = pos_tag(words)\n",
    "\n",
    "    # POS Tagging\n",
    "    print (\"Review Part of Pos tagging:\")\n",
    "\n",
    "    for i, word in enumerate(tagged):\n",
    "        print (f\"{i+1}. {word[0], {word[1]}}\")\n",
    "\n",
    "    # Synonym and Antonym\n",
    "    for word in words:\n",
    "\n",
    "        print (\"======================\")\n",
    "        print (f\"= Words : {word}\")\n",
    "        print (\"======================\")\n",
    "\n",
    "        # Synsets\n",
    "\n",
    "        synsets = wordnet.synsets(word)\n",
    "\n",
    "        synonyms = []\n",
    "        antonyms = []\n",
    "\n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.append(antonym.name())\n",
    "\n",
    "        print (\"Synonyms\")\n",
    "        if len(synonyms) == 0:\n",
    "            print (\"No synonyms detected\")\n",
    "        else:\n",
    "            for syn in synonyms[:5]:\n",
    "                print (f\"(+) : {syn}\")\n",
    "        \n",
    "        print (\"Antonyms\")\n",
    "        if len(antonyms) == 0:\n",
    "            print (\"No antonyms detected\")\n",
    "        else:\n",
    "            for ant in antonyms[:5]:\n",
    "                print (f\"(-) : {ant}\")\n",
    "\n",
    "        print(\"===========================\")\n",
    "\n",
    "    \n",
    "    # Predict the review\n",
    "\n",
    "    # Internal Preprocessing to remove punctuation and eng_stopwords and tokenize it\n",
    "\n",
    "    clean_review = [word for word in word_tokenize(review) if word not in eng_stopwords and word not in string.punctuation]\n",
    "\n",
    "    clean_review = [wnl.lemmatize(stemmer.stem(word)) for word in clean_review]\n",
    "\n",
    "    result = classifier.classify(FreqDist(clean_review))\n",
    "\n",
    "    print (f\"Your review: {review}\")\n",
    "    print (f\"Your review category: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629977ca",
   "metadata": {},
   "source": [
    "# Main Menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53da6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainMenu():\n",
    "    \n",
    "\tclassifier = read_model()\n",
    "\n",
    "\treview = \"\"\n",
    "\n",
    "\twhile True:\n",
    "\t\tprint (\"Tweet Sentiment Analysis\")\n",
    "\t\tprint (\"Your review: \", \"No Review\" if len(review) == 0 else review)\n",
    "\t\tprint (\"1. Input Review\")\n",
    "\t\tprint (\"2. Analyze Your Review\")\n",
    "\t\tprint (\"3. Exit\")\n",
    "\t\tprint (\">> \")\n",
    "\n",
    "\t\tchoice = input(\"Please input your menu choice\")\n",
    "\t\tif choice == '1':\n",
    "\t\t\treview = write_review()\n",
    "\t\telif choice == '2':\n",
    "\t\t\tanalyze_review(review, classifier)\n",
    "\t\telif choice == '3':\n",
    "\t\t\tprint (\"Thank you :)\")\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tprint (\"Input Invalid! Please choose menu choice between 1-3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa39dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is available\n",
      "Loading the model...\n",
      "Model load successfully\n",
      "Most Informative Features\n",
      "                outstand = True           positi : negati =     29.7 : 1.0\n",
      "                passbook = True           positi : negati =     29.7 : 1.0\n",
      "                 fantast = True           positi : negati =     28.7 : 1.0\n",
      "                  beauti = True           positi : negati =     27.2 : 1.0\n",
      "                 favorit = True           positi : negati =     27.2 : 1.0\n",
      "Tweet Sentiment Analysis\n",
      "Your review:  No Review\n",
      "1. Input Review\n",
      "2. Analyze Your Review\n",
      "3. Exit\n",
      ">> \n",
      "Review Added\n",
      "Tweet Sentiment Analysis\n",
      "Your review:  I not love you\n",
      "1. Input Review\n",
      "2. Analyze Your Review\n",
      "3. Exit\n",
      ">> \n",
      "Review Part of Pos tagging:\n",
      "1. ('i', {'NN'})\n",
      "2. ('not', {'RB'})\n",
      "3. ('love', {'VB'})\n",
      "4. ('you', {'PRP'})\n",
      "======================\n",
      "= Words : i\n",
      "======================\n",
      "Synonyms\n",
      "(+) : iodine\n",
      "(+) : iodin\n",
      "(+) : I\n",
      "(+) : atomic_number_53\n",
      "(+) : one\n",
      "Antonyms\n",
      "No antonyms detected\n",
      "===========================\n",
      "======================\n",
      "= Words : not\n",
      "======================\n",
      "Synonyms\n",
      "(+) : not\n",
      "(+) : non\n",
      "Antonyms\n",
      "No antonyms detected\n",
      "===========================\n",
      "======================\n",
      "= Words : love\n",
      "======================\n",
      "Synonyms\n",
      "(+) : love\n",
      "(+) : love\n",
      "(+) : passion\n",
      "(+) : beloved\n",
      "(+) : dear\n",
      "Antonyms\n",
      "(-) : hate\n",
      "(-) : hate\n",
      "===========================\n",
      "======================\n",
      "= Words : you\n",
      "======================\n",
      "Synonyms\n",
      "No synonyms detected\n",
      "Antonyms\n",
      "No antonyms detected\n",
      "===========================\n",
      "Your review: I not love you\n",
      "Your review category: positive\n",
      "Tweet Sentiment Analysis\n",
      "Your review:  I not love you\n",
      "1. Input Review\n",
      "2. Analyze Your Review\n",
      "3. Exit\n",
      ">> \n",
      "Review Part of Pos tagging:\n",
      "1. ('i', {'NN'})\n",
      "2. ('not', {'RB'})\n",
      "3. ('love', {'VB'})\n",
      "4. ('you', {'PRP'})\n",
      "======================\n",
      "= Words : i\n",
      "======================\n",
      "Synonyms\n",
      "(+) : iodine\n",
      "(+) : iodin\n",
      "(+) : I\n",
      "(+) : atomic_number_53\n",
      "(+) : one\n",
      "Antonyms\n",
      "No antonyms detected\n",
      "===========================\n",
      "======================\n",
      "= Words : not\n",
      "======================\n",
      "Synonyms\n",
      "(+) : not\n",
      "(+) : non\n",
      "Antonyms\n",
      "No antonyms detected\n",
      "===========================\n",
      "======================\n",
      "= Words : love\n",
      "======================\n",
      "Synonyms\n",
      "(+) : love\n",
      "(+) : love\n",
      "(+) : passion\n",
      "(+) : beloved\n",
      "(+) : dear\n",
      "Antonyms\n",
      "(-) : hate\n",
      "(-) : hate\n",
      "===========================\n",
      "======================\n",
      "= Words : you\n",
      "======================\n",
      "Synonyms\n",
      "No synonyms detected\n",
      "Antonyms\n",
      "No antonyms detected\n",
      "===========================\n",
      "Your review: I not love you\n",
      "Your review category: positive\n",
      "Tweet Sentiment Analysis\n",
      "Your review:  I not love you\n",
      "1. Input Review\n",
      "2. Analyze Your Review\n",
      "3. Exit\n",
      ">> \n",
      "Thank you :)\n"
     ]
    }
   ],
   "source": [
    "mainMenu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
